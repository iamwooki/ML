{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_GradientDescent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0kqelTMBgh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "#버전 2.0으로 업그레이드됨에 따라 v1과 호환이 되지 않음"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG-QknDEDVbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [1,2,3]\n",
        "y_data = [1,2,3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkoWOf6qDh97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = tf.Variable(tf.random_normal([1]), name='weight') #임의로 바꿔가면서 판단하기 위해 placeholder\n",
        "X = tf.placeholder(tf.float32)\n",
        "Y = tf.placeholder(tf.float32)\n",
        "\n",
        "hypothesis = W * X # H(x) = Wx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1t_LcXUEVlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cost/loss function\n",
        "cost = tf.reduce_sum(tf.square(hypothesis-Y)) # cost(W) = 1/m Sigma_i=1 ^ m (Wx^i - y^i)^2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11yNCpwOWlHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#-- case 1 --\n",
        "#Minimize: Gradient Descent using derivative \n",
        "#W-= Learing_rate * derivative => W:= W-a * 1/m * Sigma_i=1 ^ m (Wx^i-y^i)*x^i\n",
        "learning_rate = 0.1 # a\n",
        "gradient = tf.reduce_mean((W* X - Y) * X)\n",
        "#위의 gradient의 경우 식이 간단해서 직접 입력해줬지만 그렇지 않은 경우 case2와 같이 사용\n",
        "descent = W - learning_rate * gradient\n",
        "update = W.assign(descent)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUC1Bhh8W0Wr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "37526d46-24f4-490e-ef02-8fc8c571b3c5"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for step in range(21):\n",
        "  sess.run(update, feed_dict={X: x_data, Y: y_data})\n",
        "  print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}), sess.run(W))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.44475186 [0.82176423]\n",
            "1 0.12650721 [0.9049409]\n",
            "2 0.03598421 [0.94930184]\n",
            "3 0.010235544 [0.97296095]\n",
            "4 0.0029114408 [0.9855792]\n",
            "5 0.00082813634 [0.9923089]\n",
            "6 0.0002355604 [0.99589807]\n",
            "7 6.700621e-05 [0.9978123]\n",
            "8 1.9058218e-05 [0.99883324]\n",
            "9 5.4211228e-06 [0.9993777]\n",
            "10 1.5420082e-06 [0.9996681]\n",
            "11 4.3886047e-07 [0.999823]\n",
            "12 1.247954e-07 [0.9999056]\n",
            "13 3.5532185e-08 [0.99994963]\n",
            "14 1.0107147e-08 [0.9999731]\n",
            "15 2.893973e-09 [0.99998564]\n",
            "16 8.1490725e-10 [0.9999924]\n",
            "17 2.2998847e-10 [0.99999595]\n",
            "18 6.446044e-11 [0.99999785]\n",
            "19 1.7553958e-11 [0.99999887]\n",
            "20 5.4143356e-12 [0.9999994]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-4_4DWcaiJl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcc1fca1-f3b5-4b19-a990-49913f75bbab"
      },
      "source": [
        "#case 2\n",
        "X=[1,2,3]\n",
        "Y=[1,2,3]\n",
        "W= tf.Variable(5.0)\n",
        "hypothesis = W * X # H(x) = Wx\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(step, sess.run(W))\n",
        "  sess.run(train)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 5.0\n",
            "1 1.2666664\n",
            "2 1.0177778\n",
            "3 1.0011852\n",
            "4 1.000079\n",
            "5 1.0000052\n",
            "6 1.0000004\n",
            "7 1.0\n",
            "8 1.0\n",
            "9 1.0\n",
            "10 1.0\n",
            "11 1.0\n",
            "12 1.0\n",
            "13 1.0\n",
            "14 1.0\n",
            "15 1.0\n",
            "16 1.0\n",
            "17 1.0\n",
            "18 1.0\n",
            "19 1.0\n",
            "20 1.0\n",
            "21 1.0\n",
            "22 1.0\n",
            "23 1.0\n",
            "24 1.0\n",
            "25 1.0\n",
            "26 1.0\n",
            "27 1.0\n",
            "28 1.0\n",
            "29 1.0\n",
            "30 1.0\n",
            "31 1.0\n",
            "32 1.0\n",
            "33 1.0\n",
            "34 1.0\n",
            "35 1.0\n",
            "36 1.0\n",
            "37 1.0\n",
            "38 1.0\n",
            "39 1.0\n",
            "40 1.0\n",
            "41 1.0\n",
            "42 1.0\n",
            "43 1.0\n",
            "44 1.0\n",
            "45 1.0\n",
            "46 1.0\n",
            "47 1.0\n",
            "48 1.0\n",
            "49 1.0\n",
            "50 1.0\n",
            "51 1.0\n",
            "52 1.0\n",
            "53 1.0\n",
            "54 1.0\n",
            "55 1.0\n",
            "56 1.0\n",
            "57 1.0\n",
            "58 1.0\n",
            "59 1.0\n",
            "60 1.0\n",
            "61 1.0\n",
            "62 1.0\n",
            "63 1.0\n",
            "64 1.0\n",
            "65 1.0\n",
            "66 1.0\n",
            "67 1.0\n",
            "68 1.0\n",
            "69 1.0\n",
            "70 1.0\n",
            "71 1.0\n",
            "72 1.0\n",
            "73 1.0\n",
            "74 1.0\n",
            "75 1.0\n",
            "76 1.0\n",
            "77 1.0\n",
            "78 1.0\n",
            "79 1.0\n",
            "80 1.0\n",
            "81 1.0\n",
            "82 1.0\n",
            "83 1.0\n",
            "84 1.0\n",
            "85 1.0\n",
            "86 1.0\n",
            "87 1.0\n",
            "88 1.0\n",
            "89 1.0\n",
            "90 1.0\n",
            "91 1.0\n",
            "92 1.0\n",
            "93 1.0\n",
            "94 1.0\n",
            "95 1.0\n",
            "96 1.0\n",
            "97 1.0\n",
            "98 1.0\n",
            "99 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZITzsUfcR_J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8680221-dbf5-4382-db06-ba2ccbb088a7"
      },
      "source": [
        "#Optional compute_gradient and apply gradient\n",
        "\n",
        "X=[1,2,3]\n",
        "Y=[1,2,3]\n",
        "W= tf.Variable(5.0)\n",
        "hypothesis = W * X # H(x) = Wx\n",
        "\n",
        "gradient = tf.reduce_mean((W* X-Y)*X) *2 \n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis-Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "\n",
        "# get gradients\n",
        "gvs = optimizer.compute_gradients(cost,[W])\n",
        "# gvs 수정\n",
        "apply_graidents = optimizer.apply_gradients(gvs)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(100):\n",
        "  print(step, sess.run( [gradient, W, gvs]))\n",
        "  sess.run(apply_graidents)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [37.333332, 5.0, [(37.333336, 5.0)]]\n",
            "1 [33.84889, 4.6266665, [(33.84889, 4.6266665)]]\n",
            "2 [30.689657, 4.2881775, [(30.689657, 4.2881775)]]\n",
            "3 [27.825289, 3.981281, [(27.825289, 3.981281)]]\n",
            "4 [25.228264, 3.7030282, [(25.228264, 3.7030282)]]\n",
            "5 [22.873625, 3.4507456, [(22.873627, 3.4507456)]]\n",
            "6 [20.738754, 3.2220094, [(20.738754, 3.2220094)]]\n",
            "7 [18.803139, 3.014622, [(18.803139, 3.014622)]]\n",
            "8 [17.04818, 2.8265905, [(17.04818, 2.8265905)]]\n",
            "9 [15.457016, 2.6561089, [(15.457016, 2.6561089)]]\n",
            "10 [14.014362, 2.5015388, [(14.014362, 2.5015388)]]\n",
            "11 [12.706355, 2.3613951, [(12.706355, 2.3613951)]]\n",
            "12 [11.520428, 2.2343316, [(11.520429, 2.2343316)]]\n",
            "13 [10.445188, 2.1191273, [(10.4451885, 2.1191273)]]\n",
            "14 [9.470304, 2.0146754, [(9.4703045, 2.0146754)]]\n",
            "15 [8.586408, 1.9199723, [(8.586408, 1.9199723)]]\n",
            "16 [7.78501, 1.8341082, [(7.7850103, 1.8341082)]]\n",
            "17 [7.058409, 1.7562581, [(7.0584097, 1.7562581)]]\n",
            "18 [6.3996243, 1.6856741, [(6.399625, 1.6856741)]]\n",
            "19 [5.8023267, 1.6216779, [(5.802327, 1.6216779)]]\n",
            "20 [5.2607765, 1.5636547, [(5.2607765, 1.5636547)]]\n",
            "21 [4.769771, 1.5110469, [(4.769771, 1.5110469)]]\n",
            "22 [4.3245926, 1.4633492, [(4.3245926, 1.4633492)]]\n",
            "23 [3.9209645, 1.4201033, [(3.9209647, 1.4201033)]]\n",
            "24 [3.555008, 1.3808937, [(3.555008, 1.3808937)]]\n",
            "25 [3.2232068, 1.3453436, [(3.223207, 1.3453436)]]\n",
            "26 [2.9223745, 1.3131115, [(2.9223745, 1.3131115)]]\n",
            "27 [2.6496189, 1.2838877, [(2.6496186, 1.2838877)]]\n",
            "28 [2.4023216, 1.2573916, [(2.4023216, 1.2573916)]]\n",
            "29 [2.178105, 1.2333684, [(2.178105, 1.2333684)]]\n",
            "30 [1.9748148, 1.2115873, [(1.9748147, 1.2115873)]]\n",
            "31 [1.7904993, 1.1918392, [(1.7904994, 1.1918392)]]\n",
            "32 [1.623386, 1.1739342, [(1.6233861, 1.1739342)]]\n",
            "33 [1.4718704, 1.1577004, [(1.4718704, 1.1577004)]]\n",
            "34 [1.3344965, 1.1429818, [(1.3344965, 1.1429818)]]\n",
            "35 [1.2099432, 1.1296368, [(1.2099432, 1.1296368)]]\n",
            "36 [1.0970153, 1.1175374, [(1.0970154, 1.1175374)]]\n",
            "37 [0.99462754, 1.1065673, [(0.9946276, 1.1065673)]]\n",
            "38 [0.90179634, 1.096621, [(0.90179634, 1.096621)]]\n",
            "39 [0.81762886, 1.0876031, [(0.81762886, 1.0876031)]]\n",
            "40 [0.7413165, 1.0794268, [(0.7413165, 1.0794268)]]\n",
            "41 [0.67212707, 1.0720136, [(0.6721271, 1.0720136)]]\n",
            "42 [0.6093953, 1.0652924, [(0.6093954, 1.0652924)]]\n",
            "43 [0.5525182, 1.0591984, [(0.55251825, 1.0591984)]]\n",
            "44 [0.50094914, 1.0536731, [(0.50094914, 1.0536731)]]\n",
            "45 [0.45419374, 1.0486636, [(0.45419377, 1.0486636)]]\n",
            "46 [0.41180158, 1.0441216, [(0.41180158, 1.0441216)]]\n",
            "47 [0.37336722, 1.0400037, [(0.37336725, 1.0400037)]]\n",
            "48 [0.33851996, 1.03627, [(0.33852, 1.03627)]]\n",
            "49 [0.30692515, 1.0328848, [(0.30692515, 1.0328848)]]\n",
            "50 [0.27827826, 1.0298156, [(0.2782783, 1.0298156)]]\n",
            "51 [0.25230527, 1.0270327, [(0.25230527, 1.0270327)]]\n",
            "52 [0.2287569, 1.0245097, [(0.2287569, 1.0245097)]]\n",
            "53 [0.20740573, 1.022222, [(0.20740573, 1.022222)]]\n",
            "54 [0.18804836, 1.020148, [(0.18804836, 1.020148)]]\n",
            "55 [0.17049654, 1.0182675, [(0.17049655, 1.0182675)]]\n",
            "56 [0.15458433, 1.0165626, [(0.15458433, 1.0165626)]]\n",
            "57 [0.14015675, 1.0150168, [(0.14015675, 1.0150168)]]\n",
            "58 [0.12707591, 1.0136153, [(0.12707591, 1.0136153)]]\n",
            "59 [0.11521538, 1.0123445, [(0.11521538, 1.0123445)]]\n",
            "60 [0.10446167, 1.0111923, [(0.10446167, 1.0111923)]]\n",
            "61 [0.09471202, 1.0101477, [(0.09471202, 1.0101477)]]\n",
            "62 [0.08587202, 1.0092006, [(0.08587202, 1.0092006)]]\n",
            "63 [0.07785805, 1.0083419, [(0.07785805, 1.0083419)]]\n",
            "64 [0.07059129, 1.0075634, [(0.07059129, 1.0075634)]]\n",
            "65 [0.06400236, 1.0068574, [(0.06400236, 1.0068574)]]\n",
            "66 [0.05802846, 1.0062174, [(0.05802846, 1.0062174)]]\n",
            "67 [0.052612226, 1.005637, [(0.052612226, 1.005637)]]\n",
            "68 [0.047702473, 1.005111, [(0.047702473, 1.005111)]]\n",
            "69 [0.043249767, 1.0046339, [(0.043249767, 1.0046339)]]\n",
            "70 [0.03921318, 1.0042014, [(0.03921318, 1.0042014)]]\n",
            "71 [0.035553534, 1.0038093, [(0.035553537, 1.0038093)]]\n",
            "72 [0.032236177, 1.0034539, [(0.03223618, 1.0034539)]]\n",
            "73 [0.029227654, 1.0031315, [(0.029227655, 1.0031315)]]\n",
            "74 [0.02649951, 1.0028392, [(0.02649951, 1.0028392)]]\n",
            "75 [0.024025917, 1.0025742, [(0.024025917, 1.0025742)]]\n",
            "76 [0.021783749, 1.002334, [(0.02178375, 1.002334)]]\n",
            "77 [0.01975123, 1.0021162, [(0.019751232, 1.0021162)]]\n",
            "78 [0.017907381, 1.0019187, [(0.017907381, 1.0019187)]]\n",
            "79 [0.016236702, 1.0017396, [(0.016236704, 1.0017396)]]\n",
            "80 [0.014720838, 1.0015773, [(0.014720838, 1.0015773)]]\n",
            "81 [0.01334699, 1.00143, [(0.013346991, 1.00143)]]\n",
            "82 [0.012100856, 1.0012965, [(0.012100856, 1.0012965)]]\n",
            "83 [0.010971785, 1.0011755, [(0.010971785, 1.0011755)]]\n",
            "84 [0.0099481745, 1.0010659, [(0.0099481745, 1.0010659)]]\n",
            "85 [0.009018898, 1.0009663, [(0.009018898, 1.0009663)]]\n",
            "86 [0.008176883, 1.0008761, [(0.008176884, 1.0008761)]]\n",
            "87 [0.007413149, 1.0007943, [(0.007413149, 1.0007943)]]\n",
            "88 [0.006721576, 1.0007201, [(0.006721576, 1.0007201)]]\n",
            "89 [0.0060940585, 1.0006529, [(0.0060940585, 1.0006529)]]\n",
            "90 [0.005525271, 1.000592, [(0.0055252714, 1.000592)]]\n",
            "91 [0.0050098896, 1.0005368, [(0.0050098896, 1.0005368)]]\n",
            "92 [0.004542589, 1.0004867, [(0.004542589, 1.0004867)]]\n",
            "93 [0.0041189194, 1.0004413, [(0.0041189194, 1.0004413)]]\n",
            "94 [0.0037339528, 1.0004001, [(0.003733953, 1.0004001)]]\n",
            "95 [0.0033854644, 1.0003628, [(0.0033854644, 1.0003628)]]\n",
            "96 [0.0030694802, 1.0003289, [(0.0030694804, 1.0003289)]]\n",
            "97 [0.0027837753, 1.0002983, [(0.0027837753, 1.0002983)]]\n",
            "98 [0.0025234222, 1.0002704, [(0.0025234222, 1.0002704)]]\n",
            "99 [0.0022875469, 1.0002451, [(0.0022875469, 1.0002451)]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}